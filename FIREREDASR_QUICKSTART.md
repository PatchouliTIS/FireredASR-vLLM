# FireRedASR vLLM é›†æˆ - å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ FireRedASR çš„ vLLM é›†æˆã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
vllm-abo/
â”œâ”€â”€ vllm/model_executor/models/
â”‚   â”œâ”€â”€ fireredasr_vllm.py              # ä¸»å®ç°æ–‡ä»¶
â”‚   â”œâ”€â”€ FIREREDASR_VLLM_README.md       # è¯¦ç»†æ–‡æ¡£
â”‚   â””â”€â”€ MIGRATION_GUIDE.md              # è¿ç§»æŒ‡å—
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ fireredasr_vllm_example.py      # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_fireredasr_vllm.py         # å•å…ƒæµ‹è¯•
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ verify_fireredasr_integration.py # éªŒè¯è„šæœ¬
â””â”€â”€ FIREREDASR_QUICKSTART.md            # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5 åˆ†é’Ÿï¼‰

### 1. éªŒè¯ç¯å¢ƒ

```bash
# è¿è¡ŒéªŒè¯è„šæœ¬
python scripts/verify_fireredasr_integration.py --model-dir /path/to/your/model

# å¦‚æœæœ‰ä»»ä½•é”™è¯¯ï¼ŒæŒ‰ç…§æç¤ºä¿®å¤
```

### 2. æœ€å°ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="fireredasr",
    trust_remote_code=True,
    override_neuron_config={
        "encoder_path": "/path/to/asr_encoder.pth.tar",
        "cmvn_path": "/path/to/cmvn.ark",
        "llm_dir": "/path/to/Qwen2-7B-Instruct",
    }
)

# å‡†å¤‡è¾“å…¥
prompts = [{
    "prompt": "<|SPEECH|>",
    "multi_modal_data": {"audio": "/path/to/audio.wav"}
}]

# ç”Ÿæˆè½¬å½•
sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
outputs = llm.generate(prompts, sampling_params)

# è·å–ç»“æœ
print(outputs[0].outputs[0].text)
```

### 3. è¿è¡Œç¤ºä¾‹

```bash
# æŸ¥çœ‹å®Œæ•´ç¤ºä¾‹
python examples/fireredasr_vllm_example.py
```

## ğŸ“¦ å®‰è£…è¦æ±‚

### å¿…éœ€ä¾èµ–

```bash
pip install torch>=2.0.0
pip install transformers>=4.30.0
pip install vllm>=0.3.0
pip install fireredasr
```

### æ¨¡å‹æ–‡ä»¶

ç¡®ä¿æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š

```
model_dir/
â”œâ”€â”€ asr_encoder.pth.tar      # ASR ç¼–ç å™¨
â”œâ”€â”€ model.pth.tar            # æŠ•å½±å±‚æƒé‡
â”œâ”€â”€ cmvn.ark                 # CMVN ç»Ÿè®¡
â””â”€â”€ Qwen2-7B-Instruct/       # LLM æ¨¡å‹
    â”œâ”€â”€ config.json
    â”œâ”€â”€ tokenizer_config.json
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ *.safetensors          # æ¨¡å‹æƒé‡
```

## ğŸ”§ æ ¸å¿ƒé…ç½®

### åŸºç¡€é…ç½®

```python
llm = LLM(
    model="fireredasr",
    trust_remote_code=True,
    override_neuron_config={
        # å¿…éœ€é…ç½®
        "encoder_path": "/path/to/asr_encoder.pth.tar",
        "cmvn_path": "/path/to/cmvn.ark",
        "llm_dir": "/path/to/Qwen2-7B-Instruct",
        
        # å¯é€‰é…ç½®
        "freeze_encoder": True,      # æ˜¯å¦å†»ç»“ç¼–ç å™¨
        "freeze_llm": False,         # æ˜¯å¦å†»ç»“ LLM
        "encoder_downsample_rate": 4, # ä¸‹é‡‡æ ·ç‡
    }
)
```

### æ€§èƒ½é…ç½®

```python
# å• GPU é…ç½®
llm = LLM(
    model="fireredasr",
    tensor_parallel_size=1,
    max_num_seqs=16,              # æ‰¹å¤§å°
    gpu_memory_utilization=0.85,
)

# å¤š GPU é…ç½®
llm = LLM(
    model="fireredasr",
    tensor_parallel_size=4,       # 4-GPU å¹¶è¡Œ
    max_num_seqs=32,
    gpu_memory_utilization=0.90,
)
```

### é‡‡æ ·é…ç½®

```python
# ASR æ¨èï¼šè´ªå©ªè§£ç 
sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=100,
    repetition_penalty=1.0,
)

# Beam searchï¼ˆæ›´é«˜è´¨é‡ï¼‰
sampling_params = SamplingParams(
    best_of=5,
    use_beam_search=True,
    temperature=0.0,
    max_tokens=100,
)

# å¸¦éšæœºæ€§çš„é‡‡æ ·
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=100,
)
```

## ğŸ“ å¸¸è§ç”¨ä¾‹

### ç”¨ä¾‹ 1: å•ä¸ªéŸ³é¢‘è½¬å½•

```python
from vllm import LLM, SamplingParams

llm = LLM(model="fireredasr", override_neuron_config={...})

prompts = [{
    "prompt": "<|SPEECH|>",
    "multi_modal_data": {"audio": "audio.wav"}
}]

outputs = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=100))
print(outputs[0].outputs[0].text)
```

### ç”¨ä¾‹ 2: æ‰¹é‡è½¬å½•

```python
audio_files = ["audio1.wav", "audio2.wav", "audio3.wav"]

prompts = [
    {"prompt": "<|SPEECH|>", "multi_modal_data": {"audio": f}}
    for f in audio_files
]

outputs = llm.generate(prompts, sampling_params)

for audio, output in zip(audio_files, outputs):
    print(f"{audio}: {output.outputs[0].text}")
```

### ç”¨ä¾‹ 3: å¤„ç†åŸå§‹éŸ³é¢‘å¼ é‡

```python
import torch

# å‡è®¾å·²æœ‰éŸ³é¢‘å¼ é‡ï¼ˆä¾‹å¦‚ä»å®æ—¶æµè·å–ï¼‰
audio_tensor = torch.randn(1, 16000 * 5)  # 5 ç§’ï¼Œ16kHz

prompts = [{
    "prompt": "<|SPEECH|>",
    "multi_modal_data": {"audio": audio_tensor}
}]

outputs = llm.generate(prompts, sampling_params)
print(outputs[0].outputs[0].text)
```

### ç”¨ä¾‹ 4: å¼‚æ­¥å¤„ç†ï¼ˆé«˜å¹¶å‘ï¼‰

```python
from vllm.engine.async_llm_engine import AsyncLLMEngine
import asyncio

async def transcribe_async(audio_files):
    engine = AsyncLLMEngine.from_engine_args(...)
    
    tasks = []
    for audio_file in audio_files:
        prompt = {
            "prompt": "<|SPEECH|>",
            "multi_modal_data": {"audio": audio_file}
        }
        task = engine.generate(prompt, sampling_params)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results

# è¿è¡Œ
audio_files = ["audio1.wav", "audio2.wav", ...]
results = asyncio.run(transcribe_async(audio_files))
```

## ğŸ¯ æ€§èƒ½è°ƒä¼˜

### å»¶è¿Ÿä¼˜åŒ–ï¼ˆå•è¯·æ±‚ï¼‰

```python
llm = LLM(
    model="fireredasr",
    max_num_seqs=1,               # å•ä¸ªåºåˆ—
    gpu_memory_utilization=0.8,
    enforce_eager=True,           # ç¦ç”¨ CUDA graphï¼ˆå‡å°‘é¦–æ¬¡å»¶è¿Ÿï¼‰
)
```

### ååé‡ä¼˜åŒ–ï¼ˆæ‰¹å¤„ç†ï¼‰

```python
llm = LLM(
    model="fireredasr",
    max_num_seqs=64,              # å¤§æ‰¹é‡
    gpu_memory_utilization=0.95,
    max_model_len=2048,
)
```

### å†…å­˜ä¼˜åŒ–

```python
llm = LLM(
    model="fireredasr",
    gpu_memory_utilization=0.7,   # é™ä½å†…å­˜ä½¿ç”¨
    max_num_seqs=8,
    enable_prefix_caching=True,   # å¯ç”¨å‰ç¼€ç¼“å­˜
)
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: ImportError: FireRedASR not installed

```bash
# è§£å†³æ–¹æ¡ˆ
pip install fireredasr
```

### é—®é¢˜ 2: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

```python
# æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®
import os
encoder_path = "/path/to/asr_encoder.pth.tar"
assert os.path.exists(encoder_path), f"Encoder not found: {encoder_path}"
```

### é—®é¢˜ 3: CUDA out of memory

```python
# è§£å†³æ–¹æ¡ˆ 1: é™ä½æ‰¹å¤§å°
llm = LLM(model="fireredasr", max_num_seqs=8)

# è§£å†³æ–¹æ¡ˆ 2: é™ä½ GPU å†…å­˜åˆ©ç”¨ç‡
llm = LLM(model="fireredasr", gpu_memory_utilization=0.7)

# è§£å†³æ–¹æ¡ˆ 3: ä½¿ç”¨å¼ é‡å¹¶è¡Œ
llm = LLM(model="fireredasr", tensor_parallel_size=2)
```

### é—®é¢˜ 4: è¾“å‡ºä¸ºç©ºæˆ–ä¸æ­£ç¡®

```python
# æ£€æŸ¥ 1: éªŒè¯ speech_token_id
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("/path/to/Qwen2-7B-Instruct")
speech_token = "<|SPEECH|>"
speech_token_id = tokenizer.convert_tokens_to_ids(speech_token)
print(f"Speech token ID: {speech_token_id}")

# æ£€æŸ¥ 2: å¢åŠ  max_tokens
sampling_params = SamplingParams(max_tokens=200)  # å¢å¤§

# æ£€æŸ¥ 3: æ£€æŸ¥éŸ³é¢‘è´¨é‡
# ç¡®ä¿éŸ³é¢‘é‡‡æ ·ç‡ä¸º 16kHzï¼Œæ ¼å¼æ­£ç¡®
```

### é—®é¢˜ 5: é€Ÿåº¦æ…¢

```python
# è¯Šæ–­æ­¥éª¤
import time

# 1. é¢„çƒ­æ¨¡å‹
dummy_prompt = [{"prompt": "<|SPEECH|>", "multi_modal_data": {"audio": dummy_audio}}]
_ = llm.generate(dummy_prompt, sampling_params)

# 2. æµ‹é‡ä¸åŒé˜¶æ®µçš„æ—¶é—´
start = time.time()
outputs = llm.generate(prompts, sampling_params)
elapsed = time.time() - start
print(f"Total time: {elapsed:.2f}s")
print(f"Throughput: {len(outputs)/elapsed:.2f} requests/s")

# 3. è€ƒè™‘æ‰¹å¤„ç†
# å°†å¤šä¸ªè¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹æ¬¡
```

## ğŸ“š æ›´å¤šèµ„æº

- **è¯¦ç»†æ–‡æ¡£**: `vllm/model_executor/models/FIREREDASR_VLLM_README.md`
- **è¿ç§»æŒ‡å—**: `vllm/model_executor/models/MIGRATION_GUIDE.md`
- **ç¤ºä¾‹ä»£ç **: `examples/fireredasr_vllm_example.py`
- **å•å…ƒæµ‹è¯•**: `tests/test_fireredasr_vllm.py`
- **éªŒè¯è„šæœ¬**: `scripts/verify_fireredasr_integration.py`

## ğŸ” æ£€æŸ¥æ¸…å•

åœ¨éƒ¨ç½²å‰ï¼Œç¡®ä¿ï¼š

- [ ] Python >= 3.8
- [ ] æ‰€æœ‰ä¾èµ–å·²å®‰è£…ï¼ˆtorch, transformers, vllm, fireredasrï¼‰
- [ ] æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼ˆencoder, projector, cmvn, llmï¼‰
- [ ] éªŒè¯è„šæœ¬é€šè¿‡
- [ ] æµ‹è¯•å•ä¸ªéŸ³é¢‘è½¬å½•æ­£å¸¸
- [ ] æµ‹è¯•æ‰¹å¤„ç†æ­£å¸¸
- [ ] æ€§èƒ½æ»¡è¶³éœ€æ±‚
- [ ] å†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é‡ç”¨ LLM å®ä¾‹**: åˆå§‹åŒ–ä¸€æ¬¡ï¼Œå¤šæ¬¡ä½¿ç”¨
2. **æ‰¹å¤„ç†ä¼˜å…ˆ**: åˆå¹¶å¤šä¸ªè¯·æ±‚æé«˜ååé‡
3. **é¢„çƒ­æ¨¡å‹**: é¦–æ¬¡è°ƒç”¨å‰é¢„çƒ­ä»¥å‡å°‘å»¶è¿Ÿ
4. **ç›‘æ§æ€§èƒ½**: å®šæœŸæ£€æŸ¥ååé‡å’Œå»¶è¿ŸæŒ‡æ ‡
5. **åˆç†é…ç½®å†…å­˜**: ç•™ 10-20% GPU å†…å­˜ç»™å…¶ä»–è¿›ç¨‹
6. **ä½¿ç”¨å¼‚æ­¥ API**: é«˜å¹¶å‘åœºæ™¯ä½¿ç”¨ AsyncLLMEngine
7. **å¯ç”¨ç¼“å­˜**: ç›¸åŒéŸ³é¢‘ä¼šè‡ªåŠ¨ç¼“å­˜ç¼–ç å™¨è¾“å‡º

## ğŸš¦ ä¸‹ä¸€æ­¥

1. **è¿è¡ŒéªŒè¯è„šæœ¬**:
   ```bash
   python scripts/verify_fireredasr_integration.py --model-dir /your/model/dir
   ```

2. **æµ‹è¯•åŸºç¡€åŠŸèƒ½**:
   ```bash
   python examples/fireredasr_vllm_example.py
   ```

3. **é›†æˆåˆ°æ‚¨çš„åº”ç”¨**:
   - å‚è€ƒç¤ºä¾‹ä»£ç 
   - æ ¹æ®éœ€æ±‚è°ƒæ•´é…ç½®
   - æµ‹è¯•æ€§èƒ½å’Œå‡†ç¡®æ€§

4. **æ€§èƒ½è°ƒä¼˜**:
   - è°ƒæ•´æ‰¹å¤§å°
   - æµ‹è¯•ä¸åŒçš„é‡‡æ ·å‚æ•°
   - æ ¹æ®ç¡¬ä»¶è°ƒæ•´å¹¶è¡Œåº¦

## ğŸ“§ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼ˆFIREREDASR_VLLM_README.mdï¼‰
2. è¿è¡ŒéªŒè¯è„šæœ¬è¯Šæ–­é—®é¢˜
3. æŸ¥çœ‹å•å…ƒæµ‹è¯•äº†è§£æ­£ç¡®ç”¨æ³•
4. å‚è€ƒè¿ç§»æŒ‡å—äº†è§£ä¸åŸå§‹å®ç°çš„å·®å¼‚

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰

